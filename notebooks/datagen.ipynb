{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2efb7c",
   "metadata": {},
   "source": [
    "# Step 0: Synthetic Data Generation + Balanced Pair Generation\n",
    "This notebook creates train/test datasets and generates record pairs with a more balanced label distribution.\n",
    "- Positive pairs are formed by matching `unique` and multiple `duplicate` entries (3 duplicates per unique)\n",
    "- Negative pairs are sampled to match a 70:30 ratio (negative:positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08a16f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b75f92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "def perturb_number(val, delta=50):\n",
    "    try:\n",
    "        return int(val) + random.randint(-delta, delta)\n",
    "    except:\n",
    "        return val\n",
    "\n",
    "def generate_record(entity_id, customer_type, similar_to=None):\n",
    "    base = {\n",
    "        'Entity ID': entity_id,\n",
    "        'First Name': fake.first_name(),\n",
    "        'Last Name': fake.last_name(),\n",
    "        'Birthdate': fake.date_of_birth(minimum_age=18, maximum_age=80).strftime('%Y-%m-%d'),\n",
    "        'Gender': random.choice(['Male', 'Female']),\n",
    "        'Job': fake.job(),\n",
    "        'Email': fake.email(),\n",
    "        'Phone': fake.phone_number(),\n",
    "        'Address': fake.address().replace('\\n', ' '),\n",
    "        'City': fake.city(),\n",
    "        'Country': fake.country(),\n",
    "        'ID Number': fake.unique.random_int(min=100000, max=999999),\n",
    "        'Insurance No': fake.unique.random_int(min=100000, max=999999),\n",
    "        'Marital Status': random.choice(['Single', 'Married', 'Divorced']),\n",
    "        'Nationality': fake.country(),\n",
    "        'Notes': fake.sentence(),\n",
    "        'Customer Type': customer_type\n",
    "    }\n",
    "    if similar_to:\n",
    "        base['Last Name'] = similar_to['Last Name']\n",
    "        base['Job'] = similar_to['Job']\n",
    "        base['City'] = similar_to['City'] if random.random() > 0.5 else fake.city()\n",
    "        base['Email'] = similar_to['Email'] if random.random() > 0.6 else fake.email()\n",
    "        base['Phone'] = perturb_number(similar_to['Phone'], 20) if random.random() > 0.5 else fake.phone_number()\n",
    "        base['Insurance No'] = perturb_number(similar_to['Insurance No'], 100)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c28e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(seed, n_unique, n_duplicates_per_unique, n_similar):\n",
    "    Faker.seed(seed)\n",
    "    random.seed(seed)\n",
    "    fake.unique.clear()\n",
    "    records = []\n",
    "    for i in range(n_unique):\n",
    "        base = generate_record(i, 'unique')\n",
    "        records.append(base)\n",
    "        for _ in range(n_duplicates_per_unique):\n",
    "            dup = base.copy()\n",
    "            dup['First Name'] = fake.first_name() if random.random() > 0.5 else base['First Name']\n",
    "            dup['Email'] = fake.email() if random.random() > 0.5 else base['Email']\n",
    "            dup['Phone'] = fake.phone_number() if random.random() > 0.5 else base['Phone']\n",
    "            dup['Customer Type'] = 'duplicate'\n",
    "            records.append(dup)\n",
    "    for i in range(n_similar):\n",
    "        ref = random.choice(records[:n_unique])\n",
    "        new_id = n_unique * (1 + n_duplicates_per_unique) + i\n",
    "        records.append(generate_record(new_id, 'similar', similar_to=ref))\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b367baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(df, negative_ratio=0.7):\n",
    "    pairs = []\n",
    "    grouped = df.groupby('Entity ID')\n",
    "    for entity_id, group in grouped:\n",
    "        uniques = group[group['Customer Type'] == 'unique']\n",
    "        duplicates = group[group['Customer Type'] == 'duplicate']\n",
    "        for i in uniques.index:\n",
    "            for j in duplicates.index:\n",
    "                pairs.append({\n",
    "                    'record1_index': i,\n",
    "                    'record2_index': j,\n",
    "                    'record1_id': entity_id,\n",
    "                    'record2_id': entity_id,\n",
    "                    'is_duplicate': 1\n",
    "                })\n",
    "    # generate negatives\n",
    "    pos_count = len(pairs)\n",
    "    neg_count = int(pos_count * negative_ratio / (1 - negative_ratio))\n",
    "    all_indices = df.index.tolist()\n",
    "    while len(pairs) < pos_count + neg_count:\n",
    "        i, j = random.sample(all_indices, 2)\n",
    "        if df.loc[i, 'Entity ID'] != df.loc[j, 'Entity ID']:\n",
    "            pairs.append({\n",
    "                'record1_index': i,\n",
    "                'record2_index': j,\n",
    "                'record1_id': df.loc[i, 'Entity ID'],\n",
    "                'record2_id': df.loc[j, 'Entity ID'],\n",
    "                'is_duplicate': 0\n",
    "            })\n",
    "    return pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89b1c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved.\n",
      "Balanced pairs saved.\n"
     ]
    }
   ],
   "source": [
    "train_df = generate_dataset(seed=42, n_unique=300, n_duplicates_per_unique=3, n_similar=100)\n",
    "test_df = generate_dataset(seed=99, n_unique=60, n_duplicates_per_unique=2, n_similar=20)\n",
    "train_df.to_excel('../data/train_customers.xlsx', index=False)\n",
    "test_df.to_excel('../data/test_customers.xlsx', index=False)\n",
    "print('Datasets saved.')\n",
    "\n",
    "train_pairs = generate_pairs(train_df, negative_ratio=0.7)\n",
    "test_pairs = generate_pairs(test_df, negative_ratio=0.7)\n",
    "train_pairs.to_csv('../data/train_pairs.csv', index=False)\n",
    "test_pairs.to_csv('../data/test_pairs.csv', index=False)\n",
    "print('Balanced pairs saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e01438e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking potential leakage in binary features...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'insurance_match'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranv\\OneDrive\\py\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'insurance_match'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../output/feature_matrix.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m leak_check_cols:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     ct = pd.crosstab(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m, df[\u001b[33m'\u001b[39m\u001b[33mis_duplicate\u001b[39m\u001b[33m'\u001b[39m], normalize=\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFeature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ct)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranv\\OneDrive\\py\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranv\\OneDrive\\py\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'insurance_match'"
     ]
    }
   ],
   "source": [
    "# Quick leakage analysis: Check if any feature perfectly separates label\n",
    "print(\"\\nChecking potential leakage in binary features...\")\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "leak_check_cols = ['insurance_match', 'phone_match', 'gender_match']\n",
    "df = pd.read_csv('../output/feature_matrix.csv')\n",
    "for col in leak_check_cols:\n",
    "    ct = pd.crosstab(df[col], df['is_duplicate'], normalize='index')\n",
    "    print(f\"\\nFeature: {col}\")\n",
    "    print(ct)\n",
    "    sns.histplot(data=df, x=col, hue='is_duplicate', multiple='stack', bins=3)\n",
    "    plt.title(f\"{col} by is_duplicate\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
