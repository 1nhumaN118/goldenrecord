{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b300fd71",
   "metadata": {},
   "source": [
    "# Step 0: Regenerate Synthetic Data with Noisy Duplicates and Feature Leakage Check\n",
    "This notebook generates customer records and noisy duplicates, where key identifiers (like phone, insurance number) are slightly modified. It also includes feature leakage analysis after pair generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b288a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ec062",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "def perturb_number(val, delta=50):\n",
    "    try:\n",
    "        return int(val) + random.randint(-delta, delta)\n",
    "    except:\n",
    "        return val\n",
    "\n",
    "def generate_record(entity_id, customer_type, similar_to=None):\n",
    "    base = {\n",
    "        'Entity ID': entity_id,\n",
    "        'First Name': fake.first_name(),\n",
    "        'Last Name': fake.last_name(),\n",
    "        'Birthdate': fake.date_of_birth(minimum_age=18, maximum_age=80).strftime('%Y-%m-%d'),\n",
    "        'Gender': random.choice(['Male', 'Female']),\n",
    "        'Job': fake.job(),\n",
    "        'Email': fake.email(),\n",
    "        'Phone': fake.phone_number(),\n",
    "        'Address': fake.address().replace('\\n', ' '),\n",
    "        'City': fake.city(),\n",
    "        'Country': fake.country(),\n",
    "        'ID Number': fake.unique.random_int(min=100000, max=999999),\n",
    "        'Insurance No': fake.unique.random_int(min=100000, max=999999),\n",
    "        'Marital Status': random.choice(['Single', 'Married', 'Divorced']),\n",
    "        'Nationality': fake.country(),\n",
    "        'Notes': fake.sentence(),\n",
    "        'Customer Type': customer_type\n",
    "    }\n",
    "    if similar_to:\n",
    "        base['Last Name'] = similar_to['Last Name']\n",
    "        base['Job'] = similar_to['Job']\n",
    "        base['City'] = similar_to['City'] if random.random() > 0.5 else fake.city()\n",
    "        base['Email'] = similar_to['Email'] if random.random() > 0.6 else fake.email()\n",
    "        base['Phone'] = perturb_number(similar_to['Phone'], 20) if random.random() > 0.5 else fake.phone_number()\n",
    "        base['Insurance No'] = perturb_number(similar_to['Insurance No'], 100)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974741b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(seed, n_unique, n_duplicates_per_unique, n_similar):\n",
    "    Faker.seed(seed)\n",
    "    random.seed(seed)\n",
    "    fake.unique.clear()\n",
    "    records = []\n",
    "    for i in range(n_unique):\n",
    "        base = generate_record(i, 'unique')\n",
    "        records.append(base)\n",
    "        for _ in range(n_duplicates_per_unique):\n",
    "            records.append(generate_record(i, 'duplicate', similar_to=base))\n",
    "    for i in range(n_similar):\n",
    "        ref = random.choice(records[:n_unique])\n",
    "        new_id = n_unique * (1 + n_duplicates_per_unique) + i\n",
    "        records.append(generate_record(new_id, 'similar', similar_to=ref))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def generate_pairs(df, negative_ratio=0.7):\n",
    "    pairs = []\n",
    "    grouped = df.groupby('Entity ID')\n",
    "    for entity_id, group in grouped:\n",
    "        uniques = group[group['Customer Type'] == 'unique']\n",
    "        duplicates = group[group['Customer Type'] == 'duplicate']\n",
    "        for i in uniques.index:\n",
    "            for j in duplicates.index:\n",
    "                pairs.append({\n",
    "                    'record1_index': i,\n",
    "                    'record2_index': j,\n",
    "                    'record1_id': entity_id,\n",
    "                    'record2_id': entity_id,\n",
    "                    'is_duplicate': 1\n",
    "                })\n",
    "    pos_count = len(pairs)\n",
    "    neg_count = int(pos_count * negative_ratio / (1 - negative_ratio))\n",
    "    all_indices = df.index.tolist()\n",
    "    while len(pairs) < pos_count + neg_count:\n",
    "        i, j = random.sample(all_indices, 2)\n",
    "        if df.loc[i, 'Entity ID'] != df.loc[j, 'Entity ID']:\n",
    "            pairs.append({\n",
    "                'record1_index': i,\n",
    "                'record2_index': j,\n",
    "                'record1_id': df.loc[i, 'Entity ID'],\n",
    "                'record2_id': df.loc[j, 'Entity ID'],\n",
    "                'is_duplicate': 0\n",
    "            })\n",
    "    return pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = generate_dataset(seed=42, n_unique=300, n_duplicates_per_unique=3, n_similar=100)\n",
    "test_df = generate_dataset(seed=99, n_unique=60, n_duplicates_per_unique=2, n_similar=20)\n",
    "train_df.to_excel('data/train_customers.xlsx', index=False)\n",
    "test_df.to_excel('data/test_customers.xlsx', index=False)\n",
    "print('Noisy datasets saved.')\n",
    "\n",
    "train_pairs = generate_pairs(train_df)\n",
    "test_pairs = generate_pairs(test_df)\n",
    "train_pairs.to_csv('data/train_pairs.csv', index=False)\n",
    "test_pairs.to_csv('data/test_pairs.csv', index=False)\n",
    "print('Balanced pairs saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af868388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick leakage analysis: Check if any feature perfectly separates label\n",
    "print(\"\\nðŸ” Checking potential leakage in binary features...\")\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "leak_check_cols = ['insurance_match', 'phone_match', 'gender_match']\n",
    "df = pd.read_csv('output/feature_matrix.csv')\n",
    "for col in leak_check_cols:\n",
    "    ct = pd.crosstab(df[col], df['is_duplicate'], normalize='index')\n",
    "    print(f\"\\nFeature: {col}\")\n",
    "    print(ct)\n",
    "    sns.histplot(data=df, x=col, hue='is_duplicate', multiple='stack', bins=3)\n",
    "    plt.title(f\"{col} by is_duplicate\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}